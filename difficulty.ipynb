{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CIS 5450 Project: Difficulty Topics\n",
        "**Group Members:**\n",
        "* **Jessica Zhang**\n",
        "* **Tongxun Hu**\n",
        "* **Tingyu Lu**\n"
      ],
      "metadata": {
        "id": "G7fXCyFHPr3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 1: Imbalanced Data\n",
        "\n",
        "### **Why we used this concept**\n",
        "Our target variable, `is_cancel`, is highly imbalanced, with cancellations representing only a small fraction of all invoices.  \n",
        "Models trained on such data tend to predict the majority class (“not canceled”) almost exclusively. While this results in high overall accuracy, it performs poorly where it matters most—**identifying canceled invoices**.\n",
        "\n",
        "Earlier in the project, we mitigated imbalance using:\n",
        "\n",
        "- **Stratified splitting**, and  \n",
        "- **`class_weight='balanced'`** in Logistic Regression and Random Forest.\n",
        "\n",
        "To deepen our treatment of this issue and satisfy the Difficulty requirement, we implemented a **data-level strategy: random oversampling of the minority class**.  \n",
        "Unlike class weighting, which adjusts the model's loss function, oversampling directly modifies the training distribution so that cancellation cases become equally represented during learning.\n",
        "\n",
        "This allowed us to compare two different imbalance-handling philosophies:\n",
        "- Emphasizing minority errors through **class weighting**, and  \n",
        "- Increasing minority presence through **resampling**.\n",
        "\n",
        "### **How we implemented it**\n",
        "##### **1. Stratified Train/Test Split**\n",
        "\n",
        "During data splitting, we used stratification to maintain the original cancellation proportion in both training and test sets.  \n",
        "This prevents pathological splits where:\n",
        "\n",
        "- the minority class might be underrepresented in training (hurting learning), or  \n",
        "- nearly absent in the test set (making evaluation unreliable).\n",
        "\n",
        "This ensured all models were trained and evaluated on representative distributions.\n",
        "\n",
        "##### **2. Class Weighting in Logistic Regression and Random Forest**\n",
        "\n",
        "We used `class_weight=\"balanced\"` in both model families.  \n",
        "This adjusts the loss function so that the classifier assigns a higher penalty to misclassified cancellations.\n",
        "\n",
        "This method modifies *how the model learns* without altering the data.  \n",
        "It encourages the model to pay more attention to minority-class errors while preserving the true class distribution.\n",
        "\n",
        "##### **3. Oversampling for Logistic Regression**\n",
        "\n",
        "To study a data-level approach, we created a separate Logistic Regression model trained on an oversampled version of the training data:\n",
        "\n",
        "- We combined `X_train` and `y_train` into one DataFrame.\n",
        "- Split into majority (`is_cancel=0`) and minority (`is_cancel=1`) subsets.\n",
        "- Upsampled the minority subset *with replacement* until both classes were equally represented.\n",
        "- Reconstructed a balanced training set and trained Logistic Regression without class weights.\n",
        "- Critically, evaluation was performed on the original test set to avoid unrealistic performance inflation.\n",
        "\n",
        "### **Results & Interpretation**\n",
        "The oversampled Logistic Regression produced:\n",
        "\n",
        "- **AUC:** 0.9692  \n",
        "- **Precision (cancelled):** 0.6789  \n",
        "- **Recall (cancelled):** 0.9398  \n",
        "- **F1 (cancelled):** 0.7883  \n",
        "\n",
        "Oversampling fundamentally changed the model's behavior:\n",
        "\n",
        "- It dramatically **increased recall** on cancelled invoices (fewer missed risky orders).  \n",
        "- Precision decreased, meaning the model raised more false alarms.  \n",
        "\n",
        "This illustrates a key trade-off in imbalanced learning:\n",
        "\n",
        "- If the goal is to avoid missing potential cancellations → **oversampling is better**.  \n",
        "- If the goal is to reduce unnecessary manual review → **class weighting may be preferable**.\n",
        "\n",
        "Although oversampling slightly improved F1 compared to the weighted Logistic Regression, the **tuned Random Forest** still delivered the best combination of AUC, recall, and stable overall performance. Thus, Random Forest remained our final model."
      ],
      "metadata": {
        "id": "e1jBlYLxQcNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 2: Hyperparameter Tuning (Random Forest)\n",
        "### **Why we used this concept**\n",
        "Random Forests contain many hyperparameters that directly influence their\n",
        "capacity, generalization behavior, and stability. Parameters such as the number\n",
        "of trees, maximum depth, leaf size, and the number of features used at each\n",
        "split can dramatically change:\n",
        "\n",
        "- how much structure the forest can learn,\n",
        "- whether it overfits noisy patterns,\n",
        "- how well it separates the cancelled vs. non-cancelled invoices.\n",
        "\n",
        "Our baseline Random Forest used manually chosen settings, which produced strong\n",
        "performance but did not guarantee that we were operating near the model’s\n",
        "optimal configuration. Because Random Forest was the strongest model family in\n",
        "earlier experiments, carefully tuning it was essential both for maximizing\n",
        "predictive performance and for demonstrating a non-trivial depth of modeling\n",
        "effort consistent with the Difficulty requirement.\n",
        "\n",
        "### **How we implemented it**\n",
        "We applied **RandomizedSearchCV** to explore the interaction of the most\n",
        "impactful Random Forest hyperparameters:\n",
        "\n",
        "- **`n_estimators`** (number of trees): affects variance reduction  \n",
        "- **`max_depth`**: controls model complexity and overfitting  \n",
        "- **`min_samples_split` / `min_samples_leaf`**: regulate how deep branches can grow  \n",
        "- **`max_features`**: balances tree diversity vs. strength  \n",
        "\n",
        "We kept **`class_weight='balanced'`** to remain consistent with our imbalance\n",
        "strategy, and we used **ROC-AUC** as the tuning objective to capture overall\n",
        "ranking performance on the minority class.\n",
        "\n",
        "The tuning process involved:\n",
        "\n",
        "1. Defining a search space that spans both shallow and deep tree structures.  \n",
        "2. Running a 3-fold cross-validated randomized search over 20 sampled\n",
        "   hyperparameter configurations.  \n",
        "3. Selecting the best-performing configuration based on validation AUC.  \n",
        "4. Evaluating the tuned model on the untouched test set.\n",
        "\n",
        "### **Results & Interpretation**\n",
        "The tuned Random Forest achieved:\n",
        "\n",
        "- **Best CV AUC:** 0.9793  \n",
        "- **Test AUC:** 0.9811 (vs. 0.979 for the baseline)\n",
        "\n",
        "The improvement is modest but consistent. The main gains came from:\n",
        "\n",
        "- deeper and more flexible trees (`max_depth=8` instead of 10 or None),  \n",
        "- slightly larger leaf size (`min_samples_leaf=2`),  \n",
        "- a tuned feature subsampling rate (`max_features=0.5`),  \n",
        "- more trees (`n_estimators=400`) providing variance reduction.\n",
        "\n",
        "The nearly overlapping ROC curves indicate that the baseline model was already\n",
        "well-configured, and tuning refined it rather than fundamentally altering its\n",
        "behavior.\n",
        "\n",
        "Nevertheless, this confirms that Random Forest performance is\n",
        "**sensitive to hyperparameter choices**, and the tuned model represents the best\n",
        "version of this algorithm for our dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUfPpvutQyA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic 3: Feature Engineering for High-Cardinality Geography  \n",
        "### **Why we used this concept**\n",
        "\n",
        "Our dataset includes dozens of countries, many with very few invoices.  \n",
        "This creates a **high-cardinality categorical feature** with severe class imbalance across categories.  \n",
        "Using raw one-hot encoding would produce:\n",
        "\n",
        "- extremely sparse features,  \n",
        "- unstable coefficients for rare countries,  \n",
        "- overfitting due to low sample counts, and  \n",
        "- noisy model behavior.\n",
        "\n",
        "At the same time, exploratory analysis suggested that **country may influence cancellation likelihood**.  \n",
        "Rather than guessing how to encode geography, we applied a *statistical* approach to determine whether country differences were meaningful enough to include in the model.\n",
        "\n",
        "This led us to combine three advanced ideas:\n",
        "\n",
        "1. **Hypothesis testing** to determine whether geographic patterns are real.  \n",
        "2. **Grouping strategy** to reduce high-cardinality noise.  \n",
        "3. **Target encoding** to preserve information while avoiding sparsity.\n",
        "\n",
        "### **How we implemented it**\n",
        "\n",
        "##### **1. Hypothesis Testing: Do UK and Non-UK Behave Differently?**\n",
        "\n",
        "We used a **two-proportion z-test** to formally assess whether the cancellation rate in the United Kingdom (the dominant country) differs from that of all other countries.\n",
        "\n",
        "- Null hypothesis: UK and non-UK cancellation rates are equal  \n",
        "- Result:  \n",
        "  - *z* ≈ -3.65  \n",
        "  - *p* ≈ 0.00026  \n",
        "\n",
        "Because the p-value is extremely small, we reject the null hypothesis and conclude that **UK cancellation behavior is significantly different** from non-UK.\n",
        "\n",
        "This statistical insight guided our feature engineering decisions.\n",
        "\n",
        "##### **2. Reducing High Cardinality: UK vs Other**\n",
        "\n",
        "Since many countries appear only a handful of times, modeling them individually would:\n",
        "\n",
        "- introduce sparse dummy variables,  \n",
        "- amplify noise,  \n",
        "- weaken generalization.\n",
        "\n",
        "Guided by the hypothesis test, we grouped:\n",
        "\n",
        "- **United Kingdom** → its own category  \n",
        "- **All other countries** → `\"Other\"`\n",
        "\n",
        "This approach preserves meaningful signal while avoiding instability from low-frequency categories.\n",
        "\n",
        "##### **3. Target Encoding to Capture Numerical Geographic Signal**\n",
        "\n",
        "To retain finer-grained patterns without one-hot encoding, we computed a numerical feature:\n",
        "$$\n",
        "\\text{country_cancel_rate}(c)\n",
        "= \\frac{\\text{Cancelled Invoices in } c}{\\text{Total Invoices in } c}\n",
        "$$\n",
        "This feature represents, for each country:\n",
        "\n",
        "- the proportion of past invoices that were cancelled,  \n",
        "- a stable estimate of cancellation tendency,  \n",
        "- a smooth numeric value instead of sparse dummy variables.\n",
        "\n",
        "We then merged this numeric feature back into each invoice.\n",
        "\n",
        "Target encoding required careful design to avoid leakage:\n",
        "\n",
        "- We used historical averages, not invoice-level outcomes.  \n",
        "- No information from the current invoice's label was used to compute the encoded value.  \n",
        "- Rare categories gained stable estimates through grouping.\n",
        "\n",
        "### **Results & Interpretation**\n",
        "The combined effect of hypothesis testing, grouping, and target encoding provided:\n",
        "\n",
        "- **Improved AUC and recall**, as geographic tendencies helped the model distinguish cancellation risk.  \n",
        "- **Stability**, since the UK/Other grouping prevented noisy behavior caused by rare categories.  \n",
        "- **Interpretability**, as `country_cancel_rate` emerged as a meaningful predictor of cancellation likelihood.  \n",
        "- **Reduced overfitting**, because we avoided thousands of sparse dummy variables.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hMWIG0EXQ3sa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QfN1JBbCMuRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}